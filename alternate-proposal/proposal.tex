% === file: main.tex ===
% LaTeX project template for:
% "VARIABLE SELECTION FOR CLUSTER ANALYSIS WITH APPLICATION TO DHS DATA"
% TeX Live 2020, pdflatex + BibTeX (default)

\documentclass[12pt,a4paper]{report}

% ------------------ Encoding & language ------------------
\usepackage[utf8]{inputenc} % for TeX Live 2020 (pdflatex)
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

% ------------------ Page layout ------------------
\usepackage{geometry}
\geometry{left=30mm,right=25mm,top=30mm,bottom=30mm}
\usepackage{setspace}
\onehalfspacing % change to \doublespacing if your institution requires it

% ------------------ Graphics & logo ------------------
\usepackage{graphicx}
\graphicspath{{images/}} % place logo.png inside ./images/
\usepackage{float}
\newcommand{\universitylogo}{images/logo.png}

% ------------------ Math ------------------
\usepackage{amsmath,amssymb,amsthm}
\usepackage{bm} % bold math

% ------------------ Tables & figures ------------------
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}

% ------------------ Algorithms, code & listings ------------------
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{listings}
\lstset{basicstyle=\ttfamily\small,breaklines=true}

% ------------------ Plotting & diagrams ------------------
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.15}

% ------------------ Citations & bibliography (APA style, BibTeX) ------------------
%Import the natbib package and sets a bibliography  and citation styles
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}} %Citation-related commands


% ------------------ Hyperlinks ------------------
\usepackage[colorlinks=true,linkcolor=black,citecolor=blue,urlcolor=blue]{hyperref}

% ------------------ Utilities ------------------
\usepackage{enumitem}
\usepackage{siunitx}
\usepackage{authblk} % for author/affiliation blocks if desired

\usepackage[intoc]{nomencl} % For list of symbols
\makenomenclature


% ------------------ Title page setup ------------------
\begin{document}
	
	% ---------- COVER PAGE ----------
	\begin{titlepage}
		\begin{center}
			
			\textbf{\hspace{35pt}THE UNIVERSITY OF NIGERIA}
			\begin{minipage}{1.1\textwidth}
				
				\begin{center}
					\begin{tabular}{lcr}\\
						
						\begin{minipage}{7cm}
							\vspace{.2cm}
							\begin{center}
								\textbf{POSTGRADUATE PROGRAMME}
								
							\end{center}
						\end{minipage}
						&
						\begin{minipage}{.15\textwidth}
							\hspace*{-0.35cm}\includegraphics[scale=0.5]{logo}
						\end{minipage}
						&
						\begin{minipage}{7cm}
							\vspace{.2cm}
							\begin{center}
								
								\textbf{DEPARTMENT OF
									STATISTICS}
								\vspace{.001cm}\\
							\end{center}
						\end{minipage}
					\end{tabular}
				\end{center}
			\end{minipage}\\		
			\vspace{.4cm}
			\fbox{\begin{minipage}{15cm}\begin{center}\vspace{0.7cm}
						\textbf{\textrm{SELECTION OF VARIABLES FOR CLUSTER ANALYSIS WITH APPLICATION TO DHS DATA IN CAMEROON }}\vspace{0.5cm}	\end{center}\end{minipage}}
			
			
			\vspace{0.4cm}
			\textit{A Proposal submitted to the Department of statistics in partial fulfillment of the requirements for the award of a PhD in Applied Statistics. }\\
			
			\vspace{1.5cm}
			
			\noindent
			\begin{minipage}[t]{1.0\textwidth}
				
				\begin{center}
					\large
					\emph{By}\\
					\textsc{\bfseries AYENDOH TERRENCE SAMA} \\Reg. No. {PG/PHD/23/97892} \\\text{(M.Sc. in Probability and Statistics)}
				\end{center}
			\end{minipage}
			
			\vspace{3cm}
			
			\begin{center}
				
				\emph{SUPERVISOR} \hfill \emph{CO-SUPERVISOR} \\
				{\bfseries Dr. M S Madukaife} \hfill
				{\bfseries Dr. E O Ossai} 							
			\end{center}
			
			
		\end{center} 
		\vfill
		\begin{center}
			\textbf{February, 2025}
		\end{center}
		
	\end{titlepage}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\newpage		
	%\addcontentsline{toc}{chapter}{COPYRIGHT}
	\thispagestyle{empty}
	\null\vfill
	\begin{center}
		
		$\copyright$ Copyright by \textbf{Ayendoh Terrence Sama} 2025
		\\\vspace{0.3cm}
		All Rights Reserved.
	\end{center}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	
	
	% ---------- ABSTRACT ----------
	\begin{abstract}
		In this thesis we will introduce two procedures for variable selection
		in cluster analysis and classification rules. One will mainly be oriented to	detect the “noisy” non–informative variables, while the other will deal with multicolinearity. A forward–backward algorithm will be proposed as well to make feasible these procedures in large data sets. A small
		simulation will be performed and some real data examples are analyzed.\\
\\
		\textbf{Keywords:} Cluster Analysis, Selection of variables, Forward-backward
		algorithm.
	\end{abstract}
	
	
	\newpage
	\tableofcontents
	\listoffigures
	\listoftables
	\newpage
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\chapter*{List of Symbols}
	\addcontentsline{toc}{chapter}{List of Symbols}
	\begin{tabular}{lp{15cm}}
		$X = (X_1,\dots,X_p)$ & Random vector of $p$ variables \\
		$p$ & Total number of variables \\
		$n$ & Sample size (number of observations) \\
		$K$ & Number of clusters \\
		$f:\mathbb{R}^p \to \{1,\dots,K\}$ & Population partition function (cluster allocation rule) \\
		$G_k$ & $k$-th cluster region: $G_k = f^{-1}(k)$ \\
		$I \subset \{1,\dots,p\}$ & Index set of selected variables \\
		$d$ & Cardinality of the selected subset $I$ ($d < p$) \\
		$Y^I$ & “Blinded” vector: $Y^I_i = X_i$ if $i \in I$, else $E(X_i)$ \\
		$Z^I$ & Conditional “blinded” vector: $Z^I_i = X_i$ if $i \in I$, else $E(X_i \mid X_I)$ \\
		$h(I)$ & Population objective function: fraction of points with unchanged cluster labels when using $I$ \\
		$h_n(I)$ & Empirical version of $h(I)$ computed from data \\
		$X^*_j$ & Observation $j$ with blinded variables replaced \\
		$\bar{X}[i]$ & Sample mean of variable $i$ \\
		$m_n$ & Size of smallest cluster in a sample \\
		$r$ & Number of nearest neighbors in conditional mean estimation \\
		$\partial G_k$ & Boundary of cluster region $G_k$ \\
		$d(x, A)$ & Distance from point $x$ to set $A$ \\
		$\mathbb{I}\{\cdot\}$ & Indicator function \\
		$\mathbb{P}(\cdot)$ & Probability measure \\
		$\mathbb{E}(\cdot)$ & Expectation operator \\
	\end{tabular}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\chapter*{List of Abbreviations }
	\addcontentsline{toc}{chapter}{List of Symbols}	
	\begin{tabular}{lp{15cm}}
				
		A.M.S. & American Mathematical Society \\
		ANOVA & Analysis of Variance \\
		BIC & Bayesian Information Criterion \\
		CART & Classification and Regression Trees \\
		GKE & General Knowledge Exam \\
		i.i.d. & Independent and Identically Distributed \\
		k-NN & $k$-Nearest Neighbors \\
		MCMC & Markov Chain Monte Carlo \\
		NN & Nearest Neighbor(s) \\
		PCA & Principal Component Analysis \\
		SEL & Socioeconomic Level \\
		TSV05 & Simulated data example from Tadesse, Sha, \& Vannucci (2005) \\
	\end{tabular}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	
	
	% ------------------ MAIN SECTIONS ------------------
	\chapter{Introduction}
	\label{ch:intro}
	%---------------------Background------------------------
	\section{Background}
	
	Recent technological developments in big data have made it fairly easy	to collect data with high volume, velocity, variety, veracity, value, and variability. Thus these datasets contain a large number of variables within a single study, "large" used here subjectively. Examples of studies on these datasets include:- examinations of genetic influences in organizational psychology
	(e.g., \citealp{chi2016can}; \citealp{arvey2016genetics}),
	personality psychology (e.g., \citealp{davis2019oxytocin})
	and social psychology (e.g., \citealp{feldman2016oxytocin});
	studies on neuroscientific foundations of behaviors in management
	(e.g., \citealp{waldman2019added}) and psychiatry research
	(e.g., \citealp{sun2009elucidating}); research aiming to predict personality
	from social media footprints (e.g., \citealp{park2015automatic}); questionnaire-based studies that
	simply collected a comprehensive set of variables (e.g., \citealp{joel2017romantic}); as well as a combination of all these types of
	data (e.g., \citealp{bzdok2018machine}).
	A noteworthy advantage of high-dimensional datasets is that they provide a detailed and comprehensive view.
	Here, the definition of “many variables” is rather subjective
	and depends largely on the field of application. In behavioral
	sciences, one can think of data sets with more than 100 variables \citep{groeneveld2016can}. These types of data
	sets become increasingly common due to the fact that novel
	types of data sources are more and more often collected.
	Thus “high-dimensional” datasets are a case where number of variables (p) exceeds the number of observations (n), i.e. $p>n$. 
	When datasets are high-dimensional, they often contain variables that are either irrelevant, redundant, or contaminated with noise. These non-informative features can mask the true cluster structure, degrade the quality of partitions, and make interpretation difficult.
	
	In the context of cluster analysis – where the	intent is to group observations in such a way that those in the
	same subgroup are similar to each other, using high-dimensional data will likely result in a more accurate estimation	of subgroups and (or) a discovery of novel subgroups. In one	of the very few reported attempts to cluster high-dimensional datasets, \citet{mothi2019machine} combined clinical measures,	Behavior Research Methods
	laboratory measures, and measures derived from MRI scans
	of psychotic patients to form a combined data set, on which
	they conducted a cluster analysis and identified three subtypes of psychoses. Evidently, clustering high-dimensional
	datasets grants an opportunity to
	clarify and deepen our understanding of the heterogeneity and
	true underlying structure of the phenomena in question.
	
	
	Although research that exploits high-dimensional datasets to identify subgroups is promising, it also comes with
	challenges. One of the most compelling challenges, as stressed by a number of scholars
	(e.g., \citealp{yarkoni2017choosing}; \citealp{waldherr2017big};
	\citealp{bzdok2018machine}), is that these data sets may comprise a large amount of "irrelevant variables" \citep{fowlkes1983method}.
	They are variables	that do not separate clusters well and therefore do not define	cluster structure. These irrelevant variables may hinder subgroup discovery by masking the cluster structure under	investigation \citep{steinley2008selection}. Therefore, a cluster analysis should effectively recover the cluster structure
	while simultaneously filtering out irrelevant variables.
	
	
	
%	A general problem in clustering or classification is to find structures in a high dimensional variable space but with small data sets. It is common that in	many practical cases, the amount of variables (that should not be confused with the amount of information) is too high. This may be due to the presence of several “noisy” non–informative variables, and/or redundant information from strongly correlated variables that may produce multicolinearity. Then	the information contained in the data set could be extracted from a reduced subset of the original variables.
%	
%	A difficult task is to find out which variables are “important”, where the
%	concept of “important” should be related to the statistical procedure we are dealing with. If we are interested in cluster analysis, we would like to find the variables that explains the groups we have found. In this way, a (small) subset of variables should “explain” as best as possible, the statistical procedure in the original space (the high dimensional space). 
%	
%	Dimension reduction techniques (like principal component analysis) will produce linear combinations of the variables which are difficult to interpret unless most of the coefficients of the linear combination are negligent.
	
	Traditional clustering methods such as hierarchical clustering, \(k\)-means, and \(k\)-medoids operate under the implicit assumption that all variables contribute equally to the definition of clusters. In reality, certain variables may carry no meaningful discriminatory power, while others may be strongly correlated with more informative ones, introducing multicollinearity. This redundancy can distort the clustering space by overemphasizing specific dimensions and inflating the apparent significance of certain patterns. The challenge, therefore, lies in identifying a subset of variables that adequately explains the cluster structure, while discarding those that add little or no value to the partition.
	
	Dimension reduction techniques like Principal Component Analysis (PCA) have been employed to address high dimensionality, but these approaches create linear combinations of variables that can be difficult to interpret in substantive terms. Variable selection methods, by contrast, aim to retain a subset of the original variables, preserving interpretability while improving clustering performance. 
	
	In this study, we propose a pair of statistical procedures specifically suited for variable selection in clustering and classification. The first method targets the identification of “noisy” non-informative variables by substituting their values with a global measure such as the marginal mean, effectively “blinding” their influence. The second method extends this concept to address multicollinearity by replacing a variable’s values with conditional expectations based on the selected subset, thereby preserving local dependency structures while removing redundant information. Both procedures are grounded in the principle that a good subset of variables is one that preserves, as closely as possible, the cluster allocations obtained using the full set of variables.
		
	This work is organized as follows:- We present a synthesis of the body of work done on the problem of variable selection in clustering, where we point out the most notable studies along with gaps that we intend to fill.
	
	We also discuss the proposed procedure for solving the problem of variable selection in clustering in the methodology section. We also present the dataset which we will use for simulation in this section.
	Lastly, we outline the results we expect to obtain by the end of this study, in the last section.
	
	
	%----------------------------------------------------------------	
	\chapter{Statement of the Problem}
	\label{ch:problem}
	
	Cluster analysis has become a cornerstone of data analysis, providing a means to uncover latent structures and natural groupings in complex datasets. Its utility spans multiple disciplines, including biology, social sciences, marketing, image processing, and bioinformatics. Yet, despite its broad applicability, the reliability and interpretability of clustering results are often compromised when the datasets under investigation contain a large number of variables, many of which may be irrelevant, redundant, or noisy. In such high-dimensional data, the inclusion of non-informative variables can obscure the underlying structure of the data, distort similarity measures, and lead to unstable or misleading clustering solutions. This problem is compounded by the well-known "curse of dimensionality", whereby distances between observations become less discriminative as dimensionality increases, causing clusters to appear less distinct and making the task of identifying meaningful partitions substantially more challenging.
	
	The challenge is not merely one of computational efficiency, although high dimensionality does indeed increase the computational burden of most clustering algorithms. Rather, the core of the problem lies in the fact that many variables contribute little or no meaningful information to the definition of clusters, while others may actively mislead the clustering process. In practice, different subsets of variables may hold relevance for different cluster structures within the same dataset, and methods that treat all variables equally risk diluting or entirely masking those structures. Furthermore, redundancy among variables can create implicit weighting effects, whereby certain dimensions exert disproportionate influence on the resulting clusters, not because they contain more information, but because their repeated patterns amplify their perceived importance.
	
	The difficulty is exacerbated by the absence of external supervision in cluster analysis. Unlike in supervised learning, where model performance can be measured against known labels, clustering typically operates without a predefined ground truth, making it far less straightforward to evaluate the role and significance of each variable. Traditional model selection criteria such as the Bayesian Information Criterion (BIC) or the Gap Statistic, while widely used for determining the optimal number of clusters, are themselves sensitive to the presence of irrelevant or redundant variables. As a result, even when advanced model-based or non-parametric clustering algorithms are applied, the inclusion of extraneous features can lead to both overestimation and underestimation of the number of clusters, as well as a degradation in the clarity of the discovered structures.
	
	Over the years, numerous methods have been proposed to address the problem of variable selection for clustering, ranging from forward and backward selection strategies to penalised likelihood approaches, sparse clustering frameworks, and Bayesian variable selection models. While these methods represent important progress, they remain constrained by limitations related to scalability, assumptions about data distribution, or their ability to detect locally relevant variables. There is still no universally accepted method that balances accuracy, interpretability, and computational feasibility, particularly for large and complex datasets where variable relevance may vary across subspaces of the data.
	
	The problem is thus twofold: first, to define in precise terms what constitutes an “informative” variable for clustering in both global and local contexts; and second, to design a robust and computationally efficient method for identifying such variables without prior knowledge of the true cluster structure. Existing methods, while valuable, are constrained by assumptions about data distribution, sensitivity to noise, and limited scalability. Moreover, most approaches fail to adequately capture locally relevant variables—those that are essential for identifying clusters in subspaces of the data but not across the dataset as a whole.
	
	Addressing this problem is of both theoretical and practical importance. A solution would not only improve the accuracy and stability of clustering results, but also enhance their interpretability for domain experts, reduce computational costs, and increase the reliability of decisions derived from clustered data. This study seeks to address this gap by developing a robust procedure for variable selection in clustering that balances accuracy, interpretability, and scalability, while also accounting for local variations in variable relevance within high-dimensional datasets.
		
	
	\chapter{Aim and Objectives of the Study}
	\label{ch:aim}
	
	The problem of variable selection in cluster analysis remains both practically important and methodologically challenging. This research aims to develop a procedure for variable selection for clustering of high-dimensional data like the DHS data, that achieves a balance between computational efficiency and selection accuracy. Then we empirically validate the proposed approach on real-world datasets, like the DHS dataset for Cameroon, demonstrating improvements in interpretability, robustness, and clustering performance when compared with other standard methods like k-means and hierarchical clustering techniques.
	This will contribute to more accurate, interpretable, and computationally feasible clustering solutions in high-dimensional data.
	
	\section*{Main Objective}
	The main objective of this study is \textit{to develop a procedure which effectively and efficiently identifies and selects variables that truly contribute to the underlying cluster structure—while filtering out irrelevant or misleading features—to improve clustering accuracy, interpretability, and computational feasibility}
	\section*{Specific Objectives}
	\begin{enumerate}[label=\textbf{\arabic*:}]
		\item To review existing variable selection methods for clustering and identify limitations when applied to high dimensional data.
		\item To develop variable selection strategies suitable for mixed-type, survey-weighted DHS variables.
		\item To evaluate proposed methods through simulation and application to one or more Cameroon DHS datasets, as well as compare it with standard clustering procedures such as k-means and hierarchical clustering algorithms.		
		\item To develop an implementation of the proposed method as an R package for reproducibility and ease of use.
	\end{enumerate}
	
	\chapter{Significance of the Study}
	\label{ch:significance}
	
	The problem of selecting relevant variables in cluster analysis is of both theoretical and practical importance in the era of high-dimensional data. As data collection technologies become more advanced and accessible, researchers in fields as diverse as genomics, finance, education, and energy analytics are confronted with datasets containing hundreds or thousands of variables. In such contexts, the presence of irrelevant, noisy, or redundant variables not only undermines the accuracy of clustering procedures but also obscures the interpretability of the resulting group structures. The ability to identify a parsimonious subset of variables that retains the essential information for clustering is therefore a crucial step toward producing reliable, interpretable, and computationally efficient results.
	
	The methodology to be employed in this study offers an elegant and statistically grounded approach to this challenge. By "blinding" non-informative variables using marginal or conditional means, it becomes possible to directly assess their contribution to the clustering process, without relying on arbitrary heuristics or black-box transformations. The conditional mean extension further addresses the pervasive issue of multicollinearity, allowing the procedure to disentangle redundancy from genuine variable importance. The integration of a forward–backward search algorithm ensures that these methods remain feasible and effective even in large-scale applications where exhaustive search would be computationally prohibitive.
	
	The significance of this research extends beyond methodological innovation. From a practical standpoint, effective variable selection enhances the interpretability of cluster solutions, enabling domain experts to link clusters to meaningful real-world constructs. In applied settings such as healthcare diagnostics, market segmentation, and educational assessment, this interpretability can inform targeted interventions, policy decisions, and strategic planning. Furthermore, by reducing dimensionality without sacrificing essential information, the proposed approach lowers computational costs, making sophisticated clustering analyses more accessible to practitioners with limited computational resources.
	
	From a scientific perspective, the study contributes to bridging the gap between theoretical advances in statistical methodology and their application to real-world problems. It demonstrates that rigorous, statistically consistent procedures can be adapted into tools that are both usable and insightful for practitioners across disciplines. Ultimately, the outcomes of this work have the potential to influence best practices in unsupervised learning, encouraging the adoption of variable selection as a standard step in high-dimensional clustering workflows.

	
	\chapter{Scope of the Study}
	\label{ch:scope}
	This study focuses on the development, adaptation, and evaluation of statistical procedures for variable selection in cluster analysis. Specifically, the research will examine two complementary approaches: the marginal mean "blinding" method, designed to detect and remove non-informative noisy variables, and the conditional mean method, aimed at addressing multicollinearity and redundancy. These methods will be integrated with a forward–backward search algorithm to ensure computational feasibility in high-dimensional contexts. The primary application domain will be on high-dimensional datasets like DHS data obtained from the National Institute of Statistics in Cameroon. This will enable the demonstration of the methods’ generality and robustness.

	Simulated data may be used in this study. Performance will be evaluated in terms of classification agreement with the full-variable clustering, reduction in dimensionality, computational cost, and interpretability of the resulting variable subsets.
	
	However, several limitations should be acknowledged. First, the methods under consideration depend on the initial clustering obtained from the full set of variables. If the initial clustering is poor due to inappropriate choice of clustering algorithm, distance measure, or number of clusters, the variable selection process may propagate these deficiencies. Second, while the conditional mean method addresses multicollinearity, it requires reliable estimation of conditional expectations, which in turn demands sufficiently large sample sizes; performance may deteriorate when the sample size is small relative to the number of variables. Third, the forward–backward search algorithm, though more efficient than exhaustive search, may still be computationally intensive for extremely high-dimensional data, particularly when coupled with nonparametric conditional estimation. Finally, the evaluation of variable importance in clustering inherently lacks a ground truth in unsupervised settings, meaning that conclusions must often be drawn from indirect measures such as agreement indices or stability analyses.
	
	Within these parameters, the study aims to provide a rigorous assessment and practical adaptation of variable selection methods for clustering, contributing both to methodological development and to the growing need for interpretable and efficient unsupervised learning in high-dimensional data analysis.
	
	
	%-----------------CHAPTER: LITERATURE REVIEW--------------------------
	\chapter{Literature Review}
	\label{ch:lit}
	 
	 \section{Variable Selection Approaches}
	 Cluster analysis (unsupervised classification) seeks to partition data into homogeneous groups.  Classical clustering methods include hierarchical clustering \citep{hartigan1975}, $k$-means \citep{macqueen1967}, and model-based methods \citep{kaufman1987}, among others. 
	 Variable selection in clustering has long been recognized as important for improving interpretability and avoiding overfitting. Even when the number of variables is moderate, retaining only a subset can yield better cluster recovery (Fowlkes {\it et al}., 1988). In model-based clustering – where data are assumed to arise from a finite mixture of distributions – extraneous “noise” features can severely degrade estimates of the number of clusters and the parameter estimates. Fop and Murphy (2018) provide a comprehensive review of variable selection in this context, emphasizing that redundant or irrelevant variables increase model complexity and over-parameterization. Our goal is to survey the evolution of methods for variable selection in model-based clustering of multivariate continuous and categorical data, from early contributions to modern high-dimensional approaches.
	 
	
	 
	 \section{Introduction} Variable selection in clustering has long been recognized as important for improving interpretability and avoiding overfitting. Even when the number of variables is moderate, retaining only a subset can yield better cluster recovery (Fowlkes {\it et al}., 1988[1]). In model-based clustering – where data are assumed to arise from a finite mixture of distributions – extraneous “noise” features can severely degrade estimates of the number of clusters and the parameter estimates. Fop and Murphy (2018) provide a comprehensive review of variable selection in this context, emphasizing that redundant or irrelevant variables increase model complexity and over-parameterization[2][3]. Our goal is to survey the evolution of methods for variable selection in model-based clustering of multivariate continuous and categorical data, from early contributions to modern high-dimensional approaches.
	 \section{Model-Based Clustering Frameworks} Model-based clustering typically assumes each observation $x_i$ arises from one of $G$ groups, with group memberships indicated by a latent variable $z_i\in{1,\dots,G}$. For multivariate {\it continuous} data, a common choice is the Gaussian mixture model (GMM): $$p(x_i;\Theta)=\sum_{g=1}^G\tau_g\;\phi(x_i;\mu_g,\Sigma_g),$$ where $\phi(\cdot;\mu,\Sigma)$ is a multivariate normal density and $\tau_g$ are mixing proportions. Parameter estimation is usually by the EM algorithm. For {\it categorical} data, latent class analysis (LCA) assumes a mixture of discrete distributions (often conditional independence within classes). In both settings, the number of clusters $G$ and the covariance/association structure (e.g.\ diagonal vs full covariance) can be selected by information criteria like BIC (Schwarz, 1978). However, when many variables are available, one must decide which subset actually contains group-discriminating information. Variables may be {\it relevant} for clustering (their distributions differ across groups), {\it redundant} (correlated with relevant ones but not directly informative of group membership), or {\it irrelevant/noise} (independent of the grouping)[2][4]. This notion was formalized by Raftery and Dean (2006), who partition variables into these roles and select subsets by comparing competing mixture models[5][4]. Similarly, for LCA one often assumes a {\it local independence} condition (within-class independence of relevant variables) and selects indicators that drive the latent classes (Dean and Raftery, 2010). In practice, clustering of modern data (e.g.\ large demographic surveys) requires methods that scale to many variables and yield interpretable feature sets.
	 \section{Wrapper/Stepwise Variable Selection} A major class of methods treats variable selection as a model-selection problem within a mixture-model framework. Raftery and Dean’s (2006) pioneering “wrapper” approach compares two mixture models at each step to assess a candidate variable’s usefulness[5][4]. Concretely, suppose $X_C$ is the current set of clustering variables and $X^P$ is a new candidate. Model $M_A$ treats $X_C$ and $X^P$ jointly as informative, while model $M_B$ assumes $X^P$ is independent of the cluster label given $X_C$ (so $X^P$ adds no new group information)[4]. These are Gaussian mixtures for continuous data, with $M_B$ further including a regression term $p(X^P\mid X_C)$ for the candidate variable if it is not a clustering variable. One computes BIC for each model: $$\text{BIC}A=\text{BIC}(X_C,X^P),\quad \text{BIC}B=\text{BIC}(X^P\mid X_C).$$ If $\text{BIC}_A-\text{BIC}_B>0$, $X^P$ is deemed to improve clustering and is added[6]. The algorithm proceeds via a greedy stepwise (forward-backward) search over variables. In practice one might start with no variables and add (forward), or start with all and remove (backward), or alternate. Scrucca and Raftery (2014) implemented this in the {\tt clustvarsel} R package with both “greedy” and “headlong” search options to speed up computation on larger feature sets[7][8]. 
%	 }(X_C)+\text{BIC}_{\rm reg
	 Dean {\it et al}. (2010) adapted this idea to LCA for categorical data. In that setting, a candidate categorical indicator $Y$ is compared in two models: in one model $Y$ is conditionally independent of the latent class given already-chosen indicators (irrelevant), while in the other model $Y$ depends on the class (relevant). Dean and Raftery use a “headlong” search strategy (rapid forward inclusion/removal) guided by likelihood/BIC comparisons[9]. Empirical studies showed this method correctly identifies clustering variables and yields more parsimonious latent class models (e.g.\ the International HapMap example in Dean {\it et al}. 2010). However, this conditional-independence assumption in the null model means that truly redundant variables (correlated with $X_C$ but not directly with class) cannot be discarded by the original algorithm[10]. Subsequent work (Maugis {\it et al}., 2009; Fop {\it et al}., 2017) addressed this by allowing a candidate to depend on a subset of $X_C$ even when not a primary class predictor.
	 \section{Penalized Likelihood and Sparse Methods} An alternative “embedded” strategy is to impose a sparsity penalty on the mixture model parameters so that irrelevant variables drop out naturally. Pan and Shen (2007) introduced this in GMM clustering by adding an $L_1$ penalty on component means: $$\lambda\sum_{g=1}^G\sum_{j=1}^J|\mu_{gj}|.$$ After centering, if all $\mu_{gj}=0$ for a variable $j$, that variable’s mean is equal across components and contributes nothing to cluster separation[11]. This yields a sparse solution (some component means are exactly zero) and variable $j$ is effectively removed. A similar idea was used by Bhattacharya and McNicholas (2014) with a cluster-weighted penalty to balance cluster sizes in high dimensions. Witten and Tibshirani (2010) proposed a related sparse clustering method for K-means (not model-based) via a penalized matrix decomposition; this achieves feature selection by L1 constraints and was implemented in the {\tt sparcl} R package. In all penalized approaches, a tuning parameter (e.g.\ $\lambda$) governs the sparsity level. Cross-validation or modified BIC criteria are typically used to select it. Penalized methods naturally scale better to high-dimensional data than exhaustive wrappers, but they require careful calibration and generally treat variables independently, which can miss grouping structure or produce biased estimates.
	 \section{Bayesian Variable Selection} Bayesian frameworks treat feature selection by introducing latent indicators or priors on parameters. For instance, Kim {\it et al} (2006) used a Dirichlet process mixture for Gaussian clustering, adding a binary latent variable per feature to indicate whether it is “active” for clustering. Posterior inference (e.g.\ via Gibbs sampling) jointly estimates the number of clusters and the subset of relevant features[12]. The Dirichlet process allows the model to have an effectively infinite mixture with data-driven number of components. Storlie {\it et al}. (2018) similarly developed a hierarchical Bayesian model for mixed continuous/discrete data: discrete variables are mapped to latent normals, and both clustering and variable inclusion are inferred via a Dirichlet process mixture. Applied to autism screening data (many cognitive test scores), this yielded a 3-cluster solution with only 4 of 55 tests deemed informative[13]. More recent work combines Bayesian mixtures with variational inference for scalability. Dang {\it et al}. (2022) introduced a stochastic variational algorithm for Dirichlet-multinomial mixtures in microbiome clustering. They include an indicator for “core” taxa and use stochastic optimization to handle datasets with tens of thousands of features. SVVS (stochastic variational variable selection) can cluster over 50,000 dimensions and identify a small core set, greatly reducing computation and improving interpretability[14][15]. In general, Bayesian methods offer a principled way to quantify uncertainty in both clustering and feature inclusion, but they can be computationally intensive (often requiring MCMC or complex variational algorithms) and sensitive to prior choices.
	 \section{Information-Criterion and Model-Selection Approaches} Another line of work uses model-selection criteria directly. Raftery and Dean’s method is essentially a BIC-based wrapper, but more sophisticated criteria have been proposed. Marbac and Sedki (2016) formulated a penalized mixture model with an “integrated complete-data likelihood” (ICL) criterion. They note that standard BIC penalizes the number of parameters, requiring multiple EM fits; instead, their integrated criterion (called MICL) can be optimized without full likelihood estimation[16]. In practice, MICL reduces to comparing ICL scores of mixtures with different feature subsets. Marbac {\it et al}. (2018) extended this in the VarSelLCM R package to handle mixed data (continuous, integer, categorical with missing) under a conditional-independence mixture model[17]. These ICL-based methods can be faster than repeated EM and can select the number of clusters jointly with variables, but they rely on approximations whose accuracy in very high dimensions is less understood. In the latent class setting, Dean and Raftery (2010) use a headlong BIC difference test (as noted above)[9], while Fop {\it et al}. (2017) proposed a swap-stepwise search optimizing ICL to avoid local optima issues.
	 \section{Mixed and Heterogeneous Data} Many real-world datasets contain mixtures of continuous and categorical variables. Model-based clustering of such data often uses a product of distributions (e.g.\ Gaussian for continuous, multinomial for categorical) with a shared latent class. Variable selection here must decide which features of each type are relevant. Marbac and Sedki’s VarSelLCM implements exactly this: it assumes conditional independence given cluster and applies MICL-based selection to all variables jointly[18]. Storlie {\it et al}. (2018) took a Bayesian approach: treating discrete variables via latent normals and using a Dirichlet process mixture on the concatenated latent space[13]. In practice, mixed-data methods are relatively rare, but they are essential for surveys like DHS (which include demographics, binary indicators, counts, etc.). They benefit from conditional-independence assumptions that simplify likelihoods, but if variables are highly correlated this assumption can be violated, possibly missing complex patterns.
	 
	 \section{Empirical Review}
	 The variable selection problem in cluster analysis is not
	 a new topic and has been extensively studied since the
	 1980s. For example, Steinley and Brusco (2008b) have
	 compared the performance of eight different procedures to
	 address this problem. These approaches – most notably the
	 Variable Selection in K-Means (i.e., VS-KM; Brusco \&
	 Cradit 2001), model-based variable selection (Raftery \&
	 Dean, 2006), the Clustering Objects on Subsets of Attributes
	 (i.e., COSA; Friedman \& Meulman 2004) and the relative
	 clusterability weighting method (Steinley \& Brusco, 2008a)
	 – are well designed and have been extensively validated.
	 However, these methods are computationally prohibitive
	 in the presence of many variables, as the computational
	 demand grows exponentially with the number of variables.
	 For example, Steinley and Brusco (2008a) proposed to test
	 all subsets of variables that pass the initial screening, where
	 the theoretical maximum number of tests can be as high as
	 2J - 1 (with J indicating the number of variables in the
	 data set). Raftery and Dean (2006) and Brusco and Cradit
	 (2001) have both proposed a forward-searching strategy that
	 starts with an initial pair of two signaling variables and,
	 after searching all remaining variables, adds other signaling
	 variables one by one. This strategy, too, becomes very
	 inefficient when there are more than 100 variables.
	 Other methods are available, however, that are able to
	 simultaneously perform variable selection and clustering, with
	 reasonable computational time for large data sets with many
	 variables. They are, for example, Sparse k-means (SKM;
	 Witten  Tibshirani 2010) and Sparse Alternate Sum (SAS;
	 Arias-Castro  Pu 2017). Importantly, these methods have
	 been verified in several simulation studies to entail a
	 better performance than competing approaches, such as the
	 aforementioned COSA (Witten  Tibshirani, 2010).
	 
	 
	 The goal of clustering is to organize data into a small number
	 of homogeneous groups, thus aiding interpretation. Clustering
	 techniques have been employed in a wide range of scientific
	 fields, including biology, physics, chemistry, and psychology.
	 These techniques can broadly be classified into two categories:
	 hierarchical methods and partition methods (see Kaufman
	 and Rousseeuw, 1990; Gordon, 2008, and references therein).
	 The former typically start from a dissimilarity matrix that
	 captures differences between the objects to be clustered and
	 produce a family of cluster solutions, whose main property is
	 that any two clusters in the family are either disjoint or one is
	 a superset of the other. Various popular agglomerative algorithms, such as single, complete, and average linkage belong to
	 this class. Partition algorithms produce nonoverlapping clusters, whose defining characteristic is that distances between
	 objects belonging to the same cluster are in some sense smaller
	 than distances between objects in different clusters. The popular K-means algorithm (MacQueen, 1967) and its variants
	 are members of this class. A statistically motivated partition
	 method is model-based clustering, which models the data as a
	 sample from a Gaussian mixture distribution, with each component corresponding to a cluster (McLachlan and Basford,
	 1988). A number of extensions addressing various aspects of
	 this approach have recently appeared in the literature. For example, Banfield and Raftery (1993) generalized model-based
	 clustering to the non-Gaussian case, whereas Fraley (1993)
	 extended it to incorporate hierarchical clustering techniques.
	 The issue of variable selection in clustering, also known
	 as subspace clustering, has started receiving increased attention in the literature recently (for a review of some early
	 algorithms see Parsons, Haque, and Liu, 2004). For example, Friedman and Meulman (2004) proposed a hierarchical
	 clustering method that uncovers cluster structure on separate
	 subsets of variables; Tadesse, Sha, and Vannucci (2005) formulated the clustering problem in Bayesian terms and developed an MCMC sampler that searches for models comprised
	 of different clusters and subsets of variables; Hoff (2006) also
	 employed a Bayesian formulation based on a Polya urn model;
	 and Raftery and Dean (2006) introduced a method to sequentially compare two nested models to determine whether
	 a subset of variables should be included or excluded from the
	 current model. Some recent approaches addressing variable
	 selection are based on a regularization framework. Specifically, Pan and Shen (2006) proposed to maximize the Gaussian mixture likelihood while imposing an l1 penalty on the
	 cluster means. In addition, the means of all clusters were required to sum up to zero for each variable. This method removes variables for which all cluster means are shrunk to zero
	 and hence regarded as uninformative. Wang and Zhu (2007)
	 treated the cluster mean parameters associated with the same
	 variable as a natural “group” and proposed an adaptive l$\infty$
	 penalty and an adaptive hierarchical penalty to make use of
	 the available group information. Finally, Jornsten and Keles
	 (2008) introduced mixture models that lead to sparse cluster
	 representations in complex multifactor experiments.
	 
	 II. LITERATURE REVIEW
	 1. Manish Verma, Mauly Srivastava, Neha …” A 
	 Comparative Study of Various Clustering Algorithms 
	 in Data Mining” [5]
	 The author made a comparison between different 
	 clustering techniques. The aim was to measure the
	 algorithm which gives the best performance. It was 
	 observed that K-means is faster than all the algorithms 
	 that are mentioned in this paper. K-means and EM 
	 gives the best results than hierarchical clustering when 
	 working on huge data set. 
	 2. U. Kaymak and M. Setnes, “Extended fuzzy 
	 clustering algorithms” [6]
	 The author uses fuzzy clustering algorithm to 
	 divide dataset into clusters. Some of the issues using 
	 fussy algorithm were discussed by the author such as 
	 number and shape of clusters, division of data patterns, 
	 choosing the number of clusters in the data. Enhanced 
	 version of fussy means were given and their properties 
	 were illustrated. Examples were used to show that the 
	 enhanced algorithms does not require any additional 
	 input from the user and can determine partition of data 
	 on its own. 
	 3. Karthikeyan B., Dipu Jo George, G. Manikandan, 
	 Tony Thomas “A comparative study on k-means 
	 clustering and agglomerative hierarchical clustering,” 
	 [7]
	 The authors have done a comparative study to 
	 determine the best-suited algorithm among K-Means 
	 and Agglomerative Hierarchical Clustering. It was 
	 concluded that k-means can be best used for larger 
	 datasets with minimal runtime and memory change 
	 rate. It is also concluded that the agglomeration 
	 hierarchical clustering technique is best suited for 
	 smaller data sets because of the minimum overall 
	 memory consumption.
%	 4. S. H. Sastry, P. Babu and M. S. Prasada, “Analysis  Prediction of Sales Data in SA P-ERP System”
	 using Clustering Algorithms”, [8]
	 The authors of this paper used grouping 
	 procedures for recognizing contrast in item deals and 
	 furthermore to recognize and think about deals 
	 throughout a specific time. The interest for steel items 
	 is repeating furthermore, relies upon numerous 
	 variables like client profile, value, limits and expense 
	 issues. Creators have investigated deals information 
	 with bunching calculations like K-Means and EM 
	 (assumption augmentation) that uncovered many 
	 fascinating examples helpful for improving deals 
	 income and accomplishing higher deals volumes. K-Means and EM (segment Procedures) calculations are 
	 more qualified to assess deals information in
	 correlation with thickness based Procedures.
	 5. Soumi Ghosh, S. K. Dubey, “Comparative Analysis 
	 of K-Means…..” [9]
	 The paper includes comparison of two clustering 
	 techniques, centroid-based K-Means and representative 
	 object-based Fuzzy C-Means clustering techniques. 
	 This analysis is based on a performance evaluation with
	 these algorithms about how efficient outputs are 
	 generated. The results of this comparative research 
	 depicts that efficiency of FCM is somewhat closer to 
	 K-means. However, computation time is still longer 
	 than K-means since the fuzzy measure calculations are 
	 involved.
	 6. M.Venkat Reddy, M. Vivekananda, RUVN Satish. 
	 [10]
	 The researchers have discovered an efficient 
	 clustering technique by comparing Divisive and 
	 Agglomerative Hierarchical Clustering with K-means. 
	 The outcome of paper was that Agglomerative 
	 clustering along with k-means is the practical choice to 
	 achieve a high degree of accuracy. Divisive clustering 
	 with k-means also functions efficiently where each 
	 cluster is fixed i.e. where the initial centroids are taken 
	 in a fixed number for each cluster rather than by 
	 random selection.
	 7.. N. Sharma .“Comparison the various clustering 
	 algorithms of weka tools”.[11] 
	 The authors have compared and contrasted
	 different clustering algorithms. Weka Tool is used to 
	 implement all of the proposed algorithms. The purpose 
	 of their research is to determine which algorithm is 
	 more appropriate and efficient. DBSCAN, EM, 
	 Farthest First, OPTICS, and the K-Means algorithms 
	 are among these algorithms. They show the benefits 
	 and drawbacks of each algorithm in this study. 
	 They have demonstrated the benefits and drawbacks of 
	 each method in this paper, however based on their 
	 study, they discovered that the k-means clustering 
	 algorithm is the simplest of the algorithms and fastest 
	 algorithm to be used with large datasets.
	 
	 \section{Recent Trends and High-Dimensional Considerations} With modern “big” datasets (large $J$), scalability and interpretability become critical. Filter methods (ranking variables by marginal criteria) are less common in clustering because there is no obvious target to supervise the ranking; most efforts focus on wrapper or embedded methods. High-dimensional problems (e.g.\ genomics or image data) often use penalization or Bayesian sparsity (as above) to handle $J\gg n$. Subspace or factor-analytic mixture models (Bouveyron and Brunet-Saumard, 2014) reduce dimensionality without explicit selection, but variable selection gives direct interpretability. Parallel and stochastic algorithms are increasingly used: for example, the clustvarsel stepwise search can run inclusion/exclusion tests in parallel, and the headlong variant avoids exhaustive scans[8]. Variational Bayes (Dang {\it et al}., 2022) and stochastic EM (not reviewed here) offer further speed-ups. Across methods, the recent focus has been on {\it sparse} clustering (few variables) and {\it interpretable} models. For instance, Witten and Tibshirani’s sparse clustering yields easily interpretable feature weights, and the R Shiny interface of VarSelLCM helps users explore selected subsets.
	 \section{Discussion of Methods} Each approach has trade-offs. Wrapper/stepwise methods (e.g.\ Raftery and Dean’s, Dean {\it et al}.’s) are conceptually simple and directly tied to likelihood criteria, but they can be computationally heavy and sensitive to greedy search paths. They assume either full conditional independence (Dean et al. 2010) or model $p(X^P|X_C)$ for candidate variables, which can miss subtle dependencies[10]. Stepwise procedures also inherit instabilities of stepwise regression (different paths may yield different subsets)[19]. Penalized likelihood methods (Lasso-type) scale better to large $J$ and automatically shrink out weak variables, but require choosing tuning parameters and may bias parameter estimates. They typically assume independent penalties on variables, potentially ignoring group structure (though extensions with group Lasso exist[20]). Bayesian methods incorporate uncertainty and can handle very complex models (e.g.\ infinite mixtures), but MCMC sampling can be slow and does not parallelize easily[21][10]. Variational approximations (e.g.\ SVVS) mitigate this at some cost in accuracy. Filter methods (based on marginal statistics) are fastest but risk discarding variables that only show signal jointly. Finally, model-selection criteria like BIC/ICL unify selection with clustering but depend on asymptotic approximations and often require a search over subsets.
	 In terms of interpretability, methods that return a small feature subset (sparse clustering, wrapper selection) are generally preferred. Penalized and Bayesian methods can also yield sparsity but may produce fractional importance weights. Robustness to noise and multicollinearity varies: penalized methods tend to handle multicollinearity by shrinking or fusing coefficients (Bhattacharya and McNicholas’s penalty incorporates cluster weights[20]), while wrapper methods can explicitly recognize redundant variables if modeled (Maugis {\it et al}., 2009). Empirical comparisons (Fop and Murphy, 2018) suggest no one-size-fits-all solution; choice depends on data size, dimensionality, and the computational budget.
	 \section{Conclusions} Variable selection in model-based clustering has matured into a rich field with methods from different traditions. Foundational work by Raftery and Dean (2006) and Dean {\it et al}. (2010) demonstrated the utility of wrapper-BIC methods for continuous and categorical data, respectively[5][9]. Since then, penalized likelihood and Bayesian approaches have been developed to tackle high-dimensional settings[11][12]. Recent advances (e.g.\ variational algorithms) are making these methods feasible for truly large datasets (e.g.\ microbiome or survey data) and yielding sparse, interpretable cluster descriptions[14][22]. This review has categorized these methods by strategy (wrapper vs embedded vs Bayesian) and data type (continuous vs categorical vs mixed), and highlighted their strengths and limitations in terms of interpretability, scalability, and noise robustness. Continued research, especially on scalable algorithms and mixed-data extensions, will further enhance the applicability of variable selection in modern model-based clustering.
	 

	
	\chapter{Methodology}
	\label{ch:methodology}
	
	\section{Introduction}
	
	Variable selection for clustering and classification is intended to find the variables that simultaneously minimize the ‘within-group’ variance and maximize the ‘between-group’ variance. The combination of	these two criteria will give variables that best show separation between the desired groups. Note that the	within-group variance for each variable $j = 1, \dots, p$ can be written as


	\[
	W_j = \frac{{\sum_{g=1}^{G}}  \sum_{i=1}^{n} z_{ig}(x_{ij} - \mu_g j)^2}{n}
	\]
	where $x_{ij}$ is observation $i$ on variable $j$, $\mu_g g$ is the mean of variable $j$ in group $g$, $n$ is the number of observations, and $z_{ig}$ is a group membership indicator variable defined so that
	\[
	z_{ig} =
	\begin{cases}
		1  \text{if observation } x_{i} = (x_{i1}, . . . , x_{ip}) \text{ belongs to cluster } g, \\
		0  \text{otherwise}.
	\end{cases}
	\]
	The leftover variance within variable $j$ not accounted for by $W_j$, or $\sigma^2_j - W_j$ in the common notation, is then the leftover variance within groups. 
	In general, calculation of this value will be necessary. However,
	if the data have been standardized to have equal variance across variables, then any variable minimizing the	within-group variance is also maximizing the leftover variance.
	
	
	
	This study is based on statistical procedures for variable selection in clustering. The core idea is to identify the smallest subset of variables that preserves, as closely as possible, the clustering structure obtained when using the full set of variables. The methods rely on the concept of ``blinding''—replacing the values of certain variables so as to neutralize their influence—and on a quantitative measure of how much the resulting partition differs from the original one. This chapter describes the methods, underlying mathematical formulation, and algorithmic implementation that we will apply.
	
	Two complementary blinding strategies will be implemented:
	\begin{enumerate}
		\item \textbf{Marginal Mean Blinding} — aimed at detecting and removing non-informative noisy variables.
		\item \textbf{Conditional Mean Blinding} — designed to detect and remove both noisy and redundant (multicollinear) variables.
	\end{enumerate}
	A forward–backward search algorithm will be employed to make the procedures computationally feasible for high-dimensional data.
	
	\section{Mathematical Notation}
	
	Let $\mathbf{X} = (X_1, X_2, \dots, X_p)$ be a $p$-dimensional random vector with joint distribution $P$. Let $n$ independent realizations $\mathbf{X}_1, \dots, \mathbf{X}_n \in \mathbb{R}^p$ constitute the dataset. A clustering procedure applied to the dataset produces a \emph{partition function}:
	\[
	f : \mathbb{R}^p \to \{1, 2, \dots, K\}
	\]
	assigning each observation to one of $K$ clusters. The corresponding partition of the space is denoted by $\mathcal{G}_k = f^{-1}(k)$, $k = 1, \dots, K$.
	
	The goal is to find a subset of variables $I \subset \{1, 2, \dots, p\}$, with $|I| = d < p$, such that the clustering assignments produced using only variables in $I$ closely match those from the full set $\{1, \dots, p\}$.
	
	
	\section{Blinding Procedures}
	
	To evaluate the contribution of each variable, we consider two complementary blinding strategies:  
	
	\subsection*{Marginal Mean Blinding}
	Each variable $X_j$ to be blinded is replaced by its marginal mean, effectively removing its variability. This neutralizes the influence of variables that add random noise without contributing to cluster separation.  
	
	\subsection*{Conditional Mean Blinding}
	Each variable $X_j$ to be blinded is replaced by its conditional mean given the remaining variables. This removes not only noise but also redundant information arising from strong correlations or multicollinearity among variables.  
	
	Both procedures allow us to measure the degree to which removing a variable alters the clustering structure, thus quantifying its informativeness.  
	
	\section{Variable Selection Algorithm}
	
	An exhaustive search over all subsets of size $d$ is computationally infeasible when $p$ is large. To address this, we adopt a \emph{forward--backward search strategy}:  
	
	\begin{itemize}
		\item \textbf{Forward Step:} Iteratively add variables that most improve the preservation of the original clustering.  
		\item \textbf{Backward Step:} Remove variables that, once included, are found to be redundant or uninformative.  
		\item \textbf{Stopping Rule:} The algorithm terminates when no further improvement can be achieved without violating a predefined efficiency threshold (e.g., proportion of preserved cluster labels).  
	\end{itemize}
	
	This iterative approach ensures scalability while balancing accuracy and computational cost.  
	
	\section{Evaluation Method}
	
	The methodology will be validated through a combination of simulation studies and real-world applications.  
	
	\subsection{Simulation Studies}
	Simulation experiments will be conducted under varying conditions to test robustness:  
	\begin{itemize}
		\item Different levels of noise variables.   
		\item Sample size variation (small, medium, large $n$).  
		\item Increasing dimensionality ($p$).  
	\end{itemize}
	
	These experiments will allow us to evaluate the ability of the methodology to recover informative variables under controlled scenarios.  
	
	\subsection{Real-world Applications}
	The framework will be applied to high-dimensional real datasets such as the \textbf{Demographic and Health Survey (DHS)} dataset containing numerous socioeconomic and demographic variables.  
		
	
	\section{Performance Metrics}
	
	Performance will be assessed using the following criteria:  
	\begin{itemize}
		\item \textbf{Clustering preservation:} Proportion of labels preserved between the full-variable and reduced-variable partitions.  
		\item \textbf{Subset size:} Number of variables retained relative to $p$.  
		\item \textbf{Computational efficiency:} Runtime and memory usage of the algorithm.  
		\item \textbf{Interpretability:} Practical relevance and interpretability of the selected variables for domain experts.  
	\end{itemize}
	

	\section{Comparative Benchmarking}
	
	The proposed methodology will be benchmarked against the following existing approaches:  
	\begin{itemize}  
		\item Sparse $k$-means clustering.  
		\item Hierarchical clustering   
	\end{itemize}
	
	Statistical tests will be applied to evaluate differences in performance across methods.  
	
	\section{Justification of Method Choice}
	
	The chosen methodology offers a balance of \emph{theoretical soundness, interpretability, and scalability}.  
	
	\begin{itemize}
		\item \textbf{Marginal Mean Blinding} provides a simple and computationally efficient baseline for detecting noisy variables.  
		\item \textbf{Conditional Mean Blinding} addresses redundancy, a more challenging but critical issue in high-dimensional data.  
		\item The \textbf{forward--backward search algorithm} ensures practical feasibility without sacrificing performance.  
	\end{itemize}
	
	Together, these methods form a stable and adaptable approach to variable selection in clustering, capable of handling real-world datasets.  
	
	\section{Summary}
	
	This chapter has presented the proposed methodology for variable selection in clustering. By using blinding procedures, forward--backward search, and rigorous evaluation strategies, the research seeks to develop a framework that is accurate, interpretable, and computationally efficient.  
	

	
	\chapter{Expected Results}
	\label{ch:expectedresults}
	
	This chapter outlines the anticipated outcomes of the proposed study on variable selection for clustering. Based on the methodology described in Chapter~\ref{ch:methodology}, the research is expected to produce results that advance both the theoretical understanding and practical implementation of variable selection in high-dimensional clustering.  
	
	\section{Theoretical Contributions}
	


		\begin{itemize}
			
			\item A theoretical framework demonstrating how blinding techniques (marginal mean and conditional mean) capture noise and redundancy in different ways.  
		
			\item Development of a scalable forward--backward search algorithm for variable selection that avoids exhaustive search while retaining accuracy.  
			\item Formal analysis of the algorithm’s computational complexity and efficiency in high-dimensional settings.  
		\end{itemize}

	

	
	From controlled simulation experiments, the proposed methodology is expected to show:  
	
	\begin{itemize}
		\item \textbf{High preservation rates:} The reduced-variable clustering partitions will closely match those obtained from the full set of variables, with minimal loss of information.  
		\item \textbf{Effective noise elimination:} Marginal mean blinding will successfully identify and remove purely random variables.  
		\item \textbf{Redundancy handling:} Conditional mean blinding will outperform marginal blinding in detecting and eliminating highly correlated or multicollinear variables.  
		\item \textbf{Robustness across conditions:} Performance will remain stable under varying levels of noise, correlation structures, dimensionality ($p$), and sample size ($n$).  
	\end{itemize}
	

	
	When applied to real datasets such as Demographic and Health Survey (DHS) data and other high-dimensional data, the methodology is expected to yield:  
	
	\begin{itemize}
		\item \textbf{Smaller, interpretable subsets of variables} that are meaningful to domain experts.  
		\item \textbf{Improved cluster stability and clarity}, leading to partitions that are more robust and interpretable.  
		\item \textbf{Computational efficiency}, reducing runtime and memory requirements compared to full-variable clustering.  
	\end{itemize}
	
	
	Against existing approaches (e.g., sparse $k$-means, hierarchical clustering with selection, the proposed framework is expected to demonstrate:  
	
	\begin{itemize}
		\item \textbf{Competitive or superior clustering accuracy}, particularly in datasets with strong redundancy or high noise.  
		\item \textbf{Better balance between accuracy and interpretability}, due to the clear role of blinding procedures.  
		\item \textbf{Improved scalability}, enabling application to datasets with thousands of variables.  
	\end{itemize}
	

	
	The anticipated findings will have broader significance:  
	
	\begin{itemize}
		\item \textbf{For theory:} Establishing blinding-based selection as a principled and generalizable approach to variable selection in clustering.  
		\item \textbf{For practice:} Providing researchers and practitioners in fields such as public health, social sciences, and bioinformatics with a reliable tool for high-dimensional data exploration.  
		\item \textbf{For computation:} Contributing an efficient algorithm that can be adapted into software packages for widespread use.  
	\end{itemize}
	
	\section{Summary}
	
	In summary, the proposed research is expected to deliver:  
	\begin{itemize}
		\item A principled framework for variable selection in clustering.  
		\item An efficient and scalable algorithm that integrates marginal and conditional blinding.  
		\item Demonstrated effectiveness through both simulation studies and real-world applications.  
		\item Enhanced accuracy, interpretability, and computational feasibility compared to existing methods.  
	\end{itemize}
	
	These results will lay the foundation for more reliable and interpretable clustering in high-dimensional data, advancing both the methodology and its applications.  
	
	
	
%	\chapter{References}
	% Use BibTeX for references. See references.bib for sample entries.

	\bibliography{references}
	
	\appendix
%	\chapter{Appendix A: Example R code snippets}
	% Include sample R code for weighted clustering, variable selection etc.
	
\end{document}

